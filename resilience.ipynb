{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilience against hardware failures\n",
    "\n",
    "Scenario:  We have a cluster that partially consists of preemptible ressources.  That is, we'll have to deal with workers suddenly being shut down during computation.  While demonstrated here with a `LocalCluster`, Dask's resilience against preempted ressources is most useful with, e.g., [Dask Kubernetes](https://kubernetes.dask.org/) or [Dask Jobqueue](https://jobqueue.dask.org).\n",
    "\n",
    "Relevant docs: <http://distributed.dask.org/en/latest/resilience.html#hardware-failures>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase resilience\n",
    "\n",
    "Whenever a worker shuts down, the scheduler will increment the suspiciousness counter of _all_ tasks that were assigned (not necessarily computing) to the worker in question.  Whenever the suspiciousness of a task exceeds a certain threshold (3 by default), the task will be considered broken.  We want to compute many tasks on only a few workers with workers shutting down randomly.  So we expect the suspiciousness of all tasks to grow rapidly.  Let's increase the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:35.167455Z",
     "iopub.status.busy": "2021-10-14T16:44:35.166735Z",
     "iopub.status.idle": "2021-10-14T16:44:35.649889Z",
     "shell.execute_reply": "2021-10-14T16:44:35.650853Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 100});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:35.658843Z",
     "iopub.status.busy": "2021-10-14T16:44:35.654937Z",
     "iopub.status.idle": "2021-10-14T16:44:36.299714Z",
     "shell.execute_reply": "2021-10-14T16:44:36.300588Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import bag as db\n",
    "import os\n",
    "import random\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:36.306550Z",
     "iopub.status.busy": "2021-10-14T16:44:36.305062Z",
     "iopub.status.idle": "2021-10-14T16:44:41.549014Z",
     "shell.execute_reply": "2021-10-14T16:44:41.549594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:35849</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>1.60 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35849' processes=4 threads=4, memory=1.60 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCluster(threads_per_worker=1, n_workers=4, memory_limit=400e6)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple workload\n",
    "\n",
    "We'll multiply a range of numbers by two, add some sleep to simulate some real work, and then reduce the whole sequence of doubled numbers by summing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:41.553087Z",
     "iopub.status.busy": "2021-10-14T16:44:41.552462Z",
     "iopub.status.idle": "2021-10-14T16:44:41.561116Z",
     "shell.execute_reply": "2021-10-14T16:44:41.561887Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiply_by_two(x):\n",
    "    sleep(0.02)\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:41.573094Z",
     "iopub.status.busy": "2021-10-14T16:44:41.565096Z",
     "iopub.status.idle": "2021-10-14T16:44:41.575047Z",
     "shell.execute_reply": "2021-10-14T16:44:41.575551Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 400\n",
    "\n",
    "x = db.from_sequence(range(N), npartitions=N // 2)\n",
    "\n",
    "mults = x.map(multiply_by_two)\n",
    "\n",
    "summed = mults.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suddenly shutting down workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mark two worker process id's as non-preemptible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:41.580766Z",
     "iopub.status.busy": "2021-10-14T16:44:41.580172Z",
     "iopub.status.idle": "2021-10-14T16:44:41.593830Z",
     "shell.execute_reply": "2021-10-14T16:44:41.597185Z"
    }
   },
   "outputs": [],
   "source": [
    "all_current_workers = [w.pid for w in cluster.scheduler.workers.values()]\n",
    "non_preemptible_workers = all_current_workers[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:41.601425Z",
     "iopub.status.busy": "2021-10-14T16:44:41.600794Z",
     "iopub.status.idle": "2021-10-14T16:44:41.608109Z",
     "shell.execute_reply": "2021-10-14T16:44:41.608593Z"
    }
   },
   "outputs": [],
   "source": [
    "def kill_a_worker():\n",
    "    preemptible_workers = [\n",
    "        w.pid for w in cluster.scheduler.workers.values()\n",
    "        if w.pid not in non_preemptible_workers]\n",
    "    if preemptible_workers:\n",
    "        os.kill(random.choice(preemptible_workers), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the computation and keep shutting down workers while it's running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:41.614983Z",
     "iopub.status.busy": "2021-10-14T16:44:41.614345Z",
     "iopub.status.idle": "2021-10-14T16:44:47.747920Z",
     "shell.execute_reply": "2021-10-14T16:44:47.747034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.core - ERROR - 'tcp://127.0.0.1:38531'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/core.py\", line 597, in handle_stream\n",
      "    handler(**merge(extra, msg))\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 2684, in handle_release_data\n",
      "    ws = self.workers[worker]\n",
      "KeyError: 'tcp://127.0.0.1:38531'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils - ERROR - 'tcp://127.0.0.1:38531'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/utils.py\", line 655, in log_errors\n",
      "    yield\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 1813, in add_worker\n",
      "    await self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 2770, in handle_worker\n",
      "    await self.handle_stream(comm=comm, extra={\"worker\": worker})\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/core.py\", line 597, in handle_stream\n",
      "    handler(**merge(extra, msg))\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 2684, in handle_release_data\n",
      "    ws = self.workers[worker]\n",
      "KeyError: 'tcp://127.0.0.1:38531'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.core - ERROR - 'tcp://127.0.0.1:38531'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/core.py\", line 528, in handle_comm\n",
      "    result = await result\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 1813, in add_worker\n",
      "    await self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 2770, in handle_worker\n",
      "    await self.handle_stream(comm=comm, extra={\"worker\": worker})\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/core.py\", line 597, in handle_stream\n",
      "    handler(**merge(extra, msg))\n",
      "  File \"/usr/share/miniconda3/envs/dask-examples/lib/python3.8/site-packages/distributed/scheduler.py\", line 2684, in handle_release_data\n",
      "    ws = self.workers[worker]\n",
      "KeyError: 'tcp://127.0.0.1:38531'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "summed = client.compute(summed)\n",
    "\n",
    "while not summed.done():\n",
    "    kill_a_worker()\n",
    "    sleep(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if results match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T16:44:47.751362Z",
     "iopub.status.busy": "2021-10-14T16:44:47.750738Z",
     "iopub.status.idle": "2021-10-14T16:44:47.780471Z",
     "shell.execute_reply": "2021-10-14T16:44:47.781305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`sum(range(400))` on cluster: 159600\t(should be 159600)\n"
     ]
    }
   ],
   "source": [
    "print(f\"`sum(range({N}))` on cluster: {summed.result()}\\t(should be {N * (N-1)})\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
